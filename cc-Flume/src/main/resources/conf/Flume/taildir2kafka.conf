
######## taildir多个目录或文件监听(不支持递归)to Kafka ########
## 支持断点续传：Property：positionFile  Default：~/.flume/taildir_position.json
## JSON格式：记录每个尾文件的inode、最后位置和绝对路径

## agent组件

a1.sources = r1
a1.channels = c1
a1.sinks = k1

## sources 数据来源

# 数据源类型
a1.sources.r1.type = TAILDIR
# 文件组
a1.sources.r1.filegroups = f1
# 监听的文件或目录
a1.sources.r1.filegroups.f1 = /opt/weblogs_imitate/.*
# 控制从同一文件连续读取的批数。如果源正在跟踪多个文件，并且其中一个文件以较快的速度写入
# 它可以阻止其他文件被处理，因为繁忙的文件将在无休止的循环中读取。这种情况下，降低这个值
a1.sources.ri.maxBatchCount = 5

## channels 通道缓存类型：内存

a1.channels.c1.type = memory
a1.channels.c1.capacity = 20000
a1.channels.c1.transactionCapacity = 10000

## sinks 数据保存/输出

# 输出类型
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
# 主题
a1.sinks.k1.kafka.topic = weblogs
# 连接到的节点
a1.sinks.k1.kafka.bootstrap.servers = bd91:9092,bd92:9092,bd93:9092
# 在一个批处理中处理多少条消息。较大的批处理提高了吞吐量，同时增加了延迟
#a1.sinks.k1.kafka.flumeBatchSize = 2
# 响应级别
a1.sinks.k1.kafka.producer.acks = 1
# 设置生产者发送批次的延迟
taildir2kafka.sinks.k1.kafka.producer.linger.ms = 100
# 设置生产者发送数据使用的压缩格式
#taildir2kafka.sinks.k1.kafka.producer.compression.type = snappy

## 组件绑定

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

######## 启动Flume客户端Agent ########
flume-ng agent --conf ../conf/ --conf-file taildir2kafka.conf --name a1 -Dflume.root.logger=INFO,console
#     命令    |    配置环境    |           配置文件             | agent实例 |          控制台日志打印
#          (../flume-1.9.0/conf/)
