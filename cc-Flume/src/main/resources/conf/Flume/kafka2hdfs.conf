
######## Kafka to 文件管理系统(HDFS) ########

## agent客户端实例：a1
## 组件：source(数据来源)、sink(数据到哪)、channel(数据传输)

# source组件：r1
a1.sources = r1
# sink组件：k1
a1.sinks = k1
# channel组件：c1
a1.channels = c1

### flume-sources配置：数据来源：Kafka ###

a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource

# 一次批处理中写入通道的最大消息数
a1.sources.r1.batchSize = 5000
# 每2s写入通道，批处理写入通道的最大时间(毫秒)，当到达该时间时，批处理将被写入
a1.sources.r1.batchDurationMillis = 2000
# 连接到的Kafka集群
a1.sources.r1.kafka.bootstrap.servers = bd91:9092,bd92:9092,bd93:9092
# 连接到的主题，多个主题间使用,隔开
a1.sources.r1.kafka.topics = topic
# 通过正则表达式连接匹配主题
#a1.sources.r1.kafka.topics.regex = ^topic[0-9]$
# 指定消费组（组id改变会触发offset重置）
a1.sources.r1.kafka.consumer.group.id = group
# 对于新的消费组，从最早的位置消费
a1.sources.r1.kafka.consumer.auto.offset.reset=earliest

### flume-channels配置：通道缓存：文件 ###

a1.channels.c1.type = file
# 使用本地磁盘作为缓存
a1.channels.c1.dataDirs = /opt/flume/file-channel/data
# 检查点，自带检查，同时支持断点续传
a1.channels.c1.checkpointDir = /opt/flume/file-channel/checkpoint

### flume-sinks配置：数据保存：Hdfs ###

# 数据保存类型：hdfs文件管理系统保存
a1.sinks.k1.type = hdfs
# hdfs输出路径(不能包含:)
a1.sinks.k1.hdfs.path = hdfs://bd91/flume/events/kafka-log/%Y-%m-%d-%H
# 文件前缀
a1.sinks.k1.hdfs.filePrefix = log-
# 文件后缀
a1.sinks.k1.hdfs.fileSuffix = .txt
# 文件类型：SequenceFile、DataStream、CompressedStream
a1.sinks.k1.hdfs.fileType = DataStream
# 文件格式：Text、Writable
a1.sinks.k1.hdfs.writeFormat = Text
# ----- 控制文件大小、小文件合并 ----- #
# 按照时间滚动：每小时生成一个新文件
a1.sinks.k1.hdfs.hdfs.rollInterval = 1*60*60
# 按照文件接收的大小滚动：装满128M滚动
a1.sinks.k1.hdfs.rollSize = 134217728
# 按照接收的event条数滚动：0为不开启
a1.sinks.k1.hdfs.rollCount = 0
# 使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true

## 组件绑定

# source连接绑定channel
a1.sources.r1.channels = c1
# channel连接绑定sink
a1.sinks.k1.channel = c1


######## 启动Flume客户端Agent ########
flume-ng agent --conf ../conf/ --conf-file kafka2hdfs.conf --name a1 -Dflume.root.logger=INFO,console
#     命令     |    配置环境    |        配置文件           | agent实例 |          控制台日志打印
#           (../flume-1.9.0/conf/)
